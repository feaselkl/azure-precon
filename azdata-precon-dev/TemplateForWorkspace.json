{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "azdata-precon-dev"
		},
		"azdata-precon-dev-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'azdata-precon-dev-WorkspaceDefaultSqlServer'"
		},
		"bdadatalake_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'bdadatalake'"
		},
		"azdata-precon-dev-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://azdatapreconstoredev.dfs.core.windows.net"
		},
		"bdadatalake_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdadatalake.dfs.core.windows.net/"
		},
		"vacctkv_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://vacctkv.vault.azure.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Execute Oxygen Load Dataflow')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Oxygen2118": {},
									"DataLake": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLakeBaseDirectory')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "azdata-precon-dev-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "synapse",
						"fileSystem": "synapse"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/azdata-precon-dev-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/O2Level_2118')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "bdadatalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "*",
						"folderPath": "marsfarming_curated/O2Level/2118/01",
						"fileSystem": "synapse"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "PlotID",
						"type": "INT_32"
					},
					{
						"name": "ReportDateTime",
						"type": "TIMESTAMP_MICROS"
					},
					{
						"name": "O2Level",
						"type": "DECIMAL",
						"precision": 7,
						"scale": 6
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/bdadatalake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azdata-precon-dev-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('azdata-precon-dev-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azdata-precon-dev-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('azdata-precon-dev-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bdadatalake')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bdadatalake_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('bdadatalake_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vacctkv')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('vacctkv_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "O2Level_2118",
								"type": "DatasetReference"
							},
							"name": "Oxygen2118"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DataLakeBaseDirectory",
								"type": "DatasetReference"
							},
							"name": "DataLake"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          PlotID as integer,",
						"          ReportDateTime as timestamp,",
						"          O2Level as decimal(7,6)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'parquet',",
						"     wildcardPaths:['*.parquet']) ~> Oxygen2118",
						"Oxygen2118 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'parquet',",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> DataLake"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/O2Level_2118')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeBaseDirectory')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/000 - RUN ONCE - Create Security')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE [TestDB]\nGO\nGO\nIF NOT EXISTS\n(\n    SELECT *\n    FROM sys.symmetric_keys\n)\nBEGIN\n    CREATE MASTER KEY;\nEND\nGO\nIF NOT EXISTS\n(\n    SELECT *\n    FROM sys.database_scoped_credentials dsc\n    WHERE\n        dsc.name = N'SasTokenWrite'\n)\nBEGIN\n    CREATE DATABASE SCOPED CREDENTIAL [SasTokenWrite]\n        WITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n        SECRET = '<FILL IN SAS TOKEN HERE>';\nEND\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/001 - Create External Objects')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE [TestDB]\nGO\nIF NOT EXISTS\n(\n    SELECT *\n    FROM sys.external_data_sources ds\n    WHERE\n        ds.name = N'MarsFarmingCurated'\n)\nBEGIN\n    CREATE EXTERNAL DATA SOURCE [MarsFarmingCurated] WITH\n    (\n        LOCATION = N'https://bdadatalake.dfs.core.windows.net/synapse/marsfarming_curated',\n        CREDENTIAL = [SasTokenWrite]\n    );\nEND\nGO\nIF NOT EXISTS\n(\n    SELECT *\n    FROM sys.external_file_formats ff\n    WHERE\n        ff.name = N'ParquetFileFormat'\n)\nBEGIN\n    CREATE EXTERNAL FILE FORMAT [ParquetFileFormat] WITH\n    (\n        FORMAT_TYPE = PARQUET,\n        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n    );\nEND\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/002 - Create View')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE [TestDB]\nGO\nDROP VIEW IF EXISTS dbo.ArabilityScore;\nGO\nCREATE VIEW [dbo].[ArabilityScore] AS\nSELECT\n    arability.*,\n    arability.filepath(1) AS Year,\n    arability.filepath(2) AS Month\nFROM OPENROWSET\n(\n    BULK 'abfss://synapse@bdadatalake.dfs.core.windows.net/marsfarming_curated/ArabilityScore/*/*/*.parquet',\n    FORMAT = 'Parquet'\n)\nWITH\n(\n    -- Because these are Parquet files, the names have to\n    -- match what's in the file metadata.  Or we can remove the WITH clause!\n    PlotID INT,\n    ReportDateTime DATETIME2(0),\n    ArabilityScore DECIMAL(3,2)\n) AS [arability];\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Inspection Violations Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'synapse_azdatapreconstoredev_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [synapse_azdatapreconstoredev_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE InspectionViolation (\n\t[HSISID] bigint,\n\t[ViolationKey] nvarchar(200),\n\t[Category] nvarchar(150),\n\t[Critical] nvarchar(10),\n\t[Severity] nvarchar(50),\n\t[InspectedBy] nvarchar(200),\n\t[Comments] nvarchar(max),\n\t[PointValue] int,\n\t[ObservationType] nvarchar(255),\n\t[ViolationType] nvarchar(255),\n\t[InspectionDate] date,\n\t[StateCode] nvarchar(75),\n\t[QuestionNo] nvarchar(4),\n\t[ViolationCode] nvarchar(125),\n\t[ShortDesc] nvarchar(300),\n\t[CDCDataItem] nvarchar(500),\n\t[PermitID] int,\n\t[RestaurantName] nvarchar(4000),\n\t[InspectionKey] nvarchar(100)\n\t)\n\tWITH (\n\tLOCATION = 'wakeinspections/refined/inspection_violations/**',\n\tDATA_SOURCE = [synapse_azdatapreconstoredev_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.InspectionViolation\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Inspections Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'synapse_azdatapreconstoredev_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [synapse_azdatapreconstoredev_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE Inspection (\n\t[HSISID] bigint,\n\t[Score] int,\n\t[Description] nvarchar(max),\n\t[Type] nvarchar(150),\n\t[Inspector] nvarchar(150),\n\t[InspectionDate] date,\n\t[PermitID] int,\n\t[RestaurantName] nvarchar(4000),\n\t[InspectionKey] nvarchar(100)\n\t)\n\tWITH (\n\tLOCATION = 'wakeinspections/refined/inspections/**',\n\tDATA_SOURCE = [synapse_azdatapreconstoredev_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.Inspection\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Restaurants Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'synapse_azdatapreconstoredev_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [synapse_azdatapreconstoredev_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE Restaurant (\n\t[HSISID] bigint,\n\t[NAME] nvarchar(4000),\n\t[ADDRESS1] nvarchar(200),\n\t[ADDRESS2] nvarchar(200),\n\t[CITY] nvarchar(75),\n\t[STATE] nvarchar(20),\n\t[POSTALCODE] nvarchar(20),\n\t[PHONENUMBER] nvarchar(50),\n\t[RESTAURANTOPENDATE] date,\n\t[FACILITYTYPE] nvarchar(50),\n\t[PERMITID] int,\n\t[X] real,\n\t[Y] real,\n\t[GEOCODESTATUS] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'wakeinspections/refined/restaurants/**',\n\tDATA_SOURCE = [synapse_azdatapreconstoredev_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.Restaurant\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Violation Codes Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'synapse_azdatapreconstoredev_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [synapse_azdatapreconstoredev_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE ViolationCode (\n\t[VIOLATIONKEY] nvarchar(200),\n\t[STATECODE] nvarchar(75),\n\t[QUESTIONNO] nvarchar(4),\n\t[VIOLATIONCODE] nvarchar(125),\n\t[SHORTDESC] nvarchar(300),\n\t[CDCDATAITEM] nvarchar(500)\n\t)\n\tWITH (\n\tLOCATION = 'wakeinspections/refined/violation_codes/**',\n\tDATA_SOURCE = [synapse_azdatapreconstoredev_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.ViolationCode\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "TestDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Access Data Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "notebookrunner",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e8a9551f-7869-44ab-ba8a-ed846c010ffc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
						"name": "notebookrunner",
						"type": "Spark",
						"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"account_name = \"bdadatalake\"\r\n",
							"container_name = \"marsfarming\"\r\n",
							"relative_path = \"dailyrollups/arabilityscore\"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
							"\r\n",
							"schema = StructType([\\\r\n",
							"    StructField(\"PlotID\", IntegerType(), True),\\\r\n",
							"    StructField(\"ReportDate\", DateType(), True),\\\r\n",
							"    StructField(\"ArabilityScore\", FloatType(), True)])\r\n",
							"\r\n",
							"arability = spark.read.option('header', 'false') \\\r\n",
							"                .option('delimiter', ',') \\\r\n",
							"                .schema(schema) \\\r\n",
							"                .csv(adls_path + '/arabilityscore.csv')"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"arability.head(5)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"benford_arability = arability.select(\"ArabilityScore\").rdd.map(lambda x : x[0] * 100.0).collect()"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"len(benford_arability)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import benford as bf"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"f1d = bf.first_digits(benford_arability, digs=1)"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Refined Lake DB Tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "notebookrunner",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "999c77b3-e854-4485-9a6d-0b73b9ae9045"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
						"name": "notebookrunner",
						"type": "Spark",
						"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS default.Inspections USING Parquet LOCATION 'https://azdatapreconstoredev.dfs.core.windows.net/synapse/wakeinspections/refined/inspections/**'\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "notebookrunner",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "aa7f650d-0aef-454c-85bd-1df9f3ec6af3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
						"name": "notebookrunner",
						"type": "Spark",
						"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Load Data into the Data Lake\r\n",
							"\r\n",
							"In this notebook, we will load Wake County, North Carolina food service inspection data.  We already have raw data in our data lake but we will take this opportunity to learn more about it and refine the data for downstream use.\r\n",
							"\r\n",
							"The first thing we will need to do is import everything in the `pyspark.sql.functions` namespace."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Food Inspections Data\r\n",
							"\r\n",
							"There are several files in our dataset.  The first one we will look at includes food inspections over the course of a decade.  At least once per year, each restaurant, food truck, mobile food vendor, meat market, and even elder care facilities and schools need to be inspected to ensure that they comply with State of North Carolina laws regarding food preparation and handling.  Then, each facility gets a numeric score.  For facilities open to the general public, they need to display this score in a prominent location. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.read.format('csv') \\\r\n",
							"        .option('header',True) \\\r\n",
							"        .option('sep', ',') \\\r\n",
							"        .option('multiLine', True) \\\r\n",
							"        .load('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/raw/inspections/Food_Inspections.csv')"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Using the `head()` function, we can get an idea of the first few rows of data."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.head(10)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The `dtypes` attribute is built in and shows us the data types for each attribute on our DataFrame.  We can see that some of these attributes are not what we would have expected."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.dtypes"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"An important paradigm when working with Spark DataFrames is the concept of **immutability**.  In other words, we don't modify data in a DataFrame (or its underlying Resilient Distributed Dataset).  Instead, we apply transformation functions to the data and generate a new result.  This allows us to create a new DataFrame with what we need, while still retaining the current DataFrame in the event that we need to start over or if some other user needs it still.\r\n",
							"\r\n",
							"The `withColumn()` function lets us replace an existing column or add a new column based on some functional operation.  In this case, we will convert the `DATE_` string into an `InspectionDate` and then break out the parts of inspection date, giving us year, month, and day.\r\n",
							"\r\n",
							"We can also use the `drop()` function to remove a column altogether from a DataFrame.\r\n",
							"\r\n",
							"Note that with Python, we need to use a backslash (`\\`) to indicate that our statement is not yet finished.  If we were using Scala, that backslash would not be necessary. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_date = df.withColumn(\"InspectionDate\", to_timestamp(col(\"DATE_\"), 'yyyy/MM/dd HH:mm:ss+00')) \\\r\n",
							"            .withColumn(\"Year\", date_format(col(\"InspectionDate\"), \"yyyy\")) \\\r\n",
							"            .withColumn(\"Month\", date_format(col(\"InspectionDate\"), \"MM\")) \\\r\n",
							"            .withColumn(\"Day\", date_format(col(\"InspectionDate\"), \"dd\")) \\\r\n",
							"            .withColumn(\"Score\", col(\"SCORE\").cast('INT')) \\\r\n",
							"            .withColumn(\"HSISID\", col(\"HSISID\").cast('LONG')) \\\r\n",
							"            .drop(\"DATE_\") \\\r\n",
							"            .drop(\"PERMITID\") \\\r\n",
							"            .drop(\"OBJECTID\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Now that we have made some modifications and created a new DataFrame, let's make sure that everything looks correct."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_date.dtypes"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We can also review the top ten rows and ensure that all is well."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_date.head(10)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Extending the Story:  Restaurants\r\n",
							"\r\n",
							"Each inspection has an `HSISID`, which represents the food service facility.  We'd like to learn more about those facilities and to do so, we can import some restaurant data for the purposes of lookup.  This is a set of restaurants in Wake County and is maintained by the same department which handles food service inspections. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest = spark.read.format('csv') \\\r\n",
							"        .option('header',True) \\\r\n",
							"        .option('sep', ',') \\\r\n",
							"        .load('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/raw/restaurants/Restaurants_in_Wake_County.csv')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Reviewing the top 10 restaurants, we can see what we have available."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest.head(10)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Having reviewed our options, we can create a final restaurant DataFrame by reshaping some of the inputs."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest_final = rest.drop(\"OBJECTID\") \\\r\n",
							"                .withColumn(\"RESTAURANTOPENDATE\", to_timestamp(col(\"RESTAURANTOPENDATE\"), 'yyyy/MM/dd HH:mm:ss+00')) \\\r\n",
							"                .withColumn(\"HSISID\", col(\"HSISID\").cast('LONG')) \\\r\n",
							"                .withColumn(\"PERMITID\", col(\"PERMITID\").cast('INT')) \\\r\n",
							"                .withColumn(\"X\", col(\"X\").cast('FLOAT')) \\\r\n",
							"                .withColumn(\"Y\", col(\"Y\").cast('FLOAT'))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here is the structure of our restaurants DataFrame."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest_final.dtypes"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"And here are the first few records of it."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest.head(10)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Because we don't have any additional data to include here, we can write the restaurants to our `\\refined\\` folder.\r\n",
							"\r\n",
							"We will write the output data in Parquet format, ensuring that we retain data types, data lengths, and column names.  That way, downstream users will automatically get the fruits of our labor here."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"rest_final.write \\\r\n",
							"          .mode(\"overwrite\") \\\r\n",
							"          .parquet('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/restaurants/')"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Violation Codes\r\n",
							"\r\n",
							"Almost every inspection will include at least one violation.  This violation codes table represents a lookup table of the various violation codes in our dataset."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vc = spark.read.format('csv') \\\r\n",
							"        .option('header',True) \\\r\n",
							"        .option('sep', ',') \\\r\n",
							"        .load('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/raw/violation_codes/Violation_Code.csv')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vc.dtypes"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vc.head(10)"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"This data is already in reasonable condition, so we can write it directly to the `\\refined\\` directory without any additional work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"vc.write \\\r\n",
							"  .mode(\"overwrite\") \\\r\n",
							"  .parquet('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/violation_codes/')"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Enhancing Inspections\r\n",
							"\r\n",
							"As a quick reminder, here is what we have on the inspections DataFrame."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_date.dtypes"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We can see an `HSISID`, which we know ties to the food service facility.  Using the `join()` function, we can join our inspection data to the restaurants DataFrame.  Note that we perform a left outer join here, as we don't know for sure that every inspection ties to a restaurant in the current dataset--it is possible that some restaurants do not show up in the dataset for whatever reason."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_inspections = df_date.join(rest_final, ['HSISID'], \"left\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The process of joining brings together the full set of attributes from both sides, making this a bit different from a classic SQL join operation."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_inspections.dtypes"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Let's narrow down the set of columns.  We'll keep restaurant name and permit ID from the restaurants DataFrame but drop most of the rest.  Also, we will change the casing on several columns and create a new column called `InspectionKey`, which gives us a unique identifier for an inspection.  We use `concat_ws()` (concatenate with separator) to create a key based on the facility ID and the date of inspection, with each ID or date part separated by underscores."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_inspectionsfinal = df_inspections \\\r\n",
							"                        .withColumn(\"RestaurantName\", col(\"NAME\")) \\\r\n",
							"                        .withColumn(\"PermitID\", col(\"PERMITID\")) \\\r\n",
							"                        .withColumn(\"InspectionKey\", concat_ws(\"_\", col(\"HSISID\"), col(\"Year\"), col(\"Month\"), col(\"Day\"))) \\\r\n",
							"                        .withColumn(\"Description\", col(\"DESCRIPTION\")) \\\r\n",
							"                        .withColumn(\"Type\", col(\"TYPE\")) \\\r\n",
							"                        .withColumn(\"Inspector\", col(\"INSPECTOR\")) \\\r\n",
							"                        .drop(\"NAME\") \\\r\n",
							"                        .drop(\"ADDRESS1\") \\\r\n",
							"                        .drop(\"ADDRESS2\") \\\r\n",
							"                        .drop(\"CITY\") \\\r\n",
							"                        .drop(\"STATE\") \\\r\n",
							"                        .drop(\"POSTALCODE\") \\\r\n",
							"                        .drop(\"PHONENUMBER\") \\\r\n",
							"                        .drop(\"RESTAURANTOPENDATE\") \\\r\n",
							"                        .drop(\"FACILITYTYPE\") \\\r\n",
							"                        .drop(\"X\") \\\r\n",
							"                        .drop(\"Y\") \\\r\n",
							"                        .drop(\"GEOCODESTATUS\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here is what our inspections data looks like now."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_inspectionsfinal.head(10)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"With this in place, we can call our refining process complete and will write out data.  Note that we partition this by year and month, so in the `\\refined\\inspections\\` folder, we will be able to drill into each year and month, so for example, `\\refined\\inspections\\2018\\04\\` would include all restaurant inspections for April of 2018."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_inspectionsfinal.write.partitionBy(\"Year\", \"Month\") \\\r\n",
							"  .mode(\"overwrite\") \\\r\n",
							"  .parquet('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/inspections/')"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Inspection Violations\r\n",
							"\r\n",
							"The actual inspection violations are in a different file from the inspections themselves and there is a one to zero or more relationship between inspections and violations.  Typically, this relationship is one to many."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_iv = spark.read.format('csv') \\\r\n",
							"        .option('header',True) \\\r\n",
							"        .option('sep', ',') \\\r\n",
							"        .option('multiLine', True) \\\r\n",
							"        .load('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/raw/inspection_violations/Food_Inspection_Violations.csv')"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here is the shape of the inspection data as it comes in."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_iv.dtypes"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Based on our experience with other datasets, we can already perform some operations, like creating an inspection date, casting different attributes to known data types, and dropping other columns.  We will drop everything associated with the violation code here."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivkey = df_iv \\\r\n",
							"            .withColumn(\"HSISID\", col(\"HSISID\").cast('LONG')) \\\r\n",
							"            .withColumn(\"InspectionDate\", to_timestamp(col(\"INSPECTDATE\"), 'yyyy/MM/dd HH:mm:ss+00')) \\\r\n",
							"            .withColumn(\"Year\", date_format(col(\"InspectionDate\"), \"yyyy\")) \\\r\n",
							"            .withColumn(\"Month\", date_format(col(\"InspectionDate\"), \"MM\")) \\\r\n",
							"            .withColumn(\"Day\", date_format(col(\"InspectionDate\"), \"dd\")) \\\r\n",
							"            .withColumn(\"POINTVALUE\", col(\"POINTVALUE\").cast('INT')) \\\r\n",
							"            .withColumn(\"VIOLATIONKEY\", concat_ws(\"_\", col(\"STATECODE\"), col(\"QUESTIONNO\"), col(\"VIOLATIONCODE\"))) \\\r\n",
							"            .drop(\"OBJECTID\") \\\r\n",
							"            .drop(\"INSPECTDATE\") \\\r\n",
							"            .drop(\"PERMITID\") \\\r\n",
							"            .drop(\"STATECODE\") \\\r\n",
							"            .drop(\"QUESTIONNO\") \\\r\n",
							"            .drop(\"VIOLATIONCODE\") \\\r\n",
							"            .drop(\"CDCRISKFACTOR\") \\\r\n",
							"            .drop(\"SHORTDESC\") \\\r\n",
							"            .drop(\"CDCDATAITEM\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivkey.head(10)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We can perform multiple joins, bringing together data from several tables.  In this case, we drive from inspection violations and perform left outer joints on violation code (`vc`) and restaurants (`rest_final`) based on their key columns."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivjoin = df_ivkey.join(vc, ['VIOLATIONKEY'], \"left\").join(rest_final, ['HSISID'], \"left\")"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Now let's look at what we've done."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivjoin.head(10)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivjoin.dtypes"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The end result is that we have a large number of columns.  Therefore, we'll want to prune them down and include only the columns we intend to keep."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivfinal = df_ivjoin \\\r\n",
							"                .withColumn(\"RestaurantName\", col(\"NAME\")) \\\r\n",
							"                .withColumn(\"PermitID\", col(\"PERMITID\")) \\\r\n",
							"                .withColumn(\"ViolationKey\", col(\"VIOLATIONKEY\")) \\\r\n",
							"                .withColumn(\"Critical\", col(\"CRITICAL\")) \\\r\n",
							"                .withColumn(\"Category\", col(\"CATEGORY\")) \\\r\n",
							"                .withColumn(\"Severity\", col(\"SEVERITY\")) \\\r\n",
							"                .withColumn(\"InspectedBy\", col(\"INSPECTEDBY\")) \\\r\n",
							"                .withColumn(\"Comments\", col(\"COMMENTS\")) \\\r\n",
							"                .withColumn(\"PointValue\", col(\"POINTVALUE\")) \\\r\n",
							"                .withColumn(\"ObservationType\", col(\"OBSERVATIONTYPE\")) \\\r\n",
							"                .withColumn(\"ViolationType\", col(\"VIOLATIONTYPE\")) \\\r\n",
							"                .withColumn(\"StateCode\", col(\"STATECODE\")) \\\r\n",
							"                .withColumn(\"QuestionNo\", col(\"QUESTIONNO\")) \\\r\n",
							"                .withColumn(\"ViolationCode\", col(\"VIOLATIONCODE\")) \\\r\n",
							"                .withColumn(\"ShortDesc\", col(\"SHORTDESC\")) \\\r\n",
							"                .withColumn(\"CDCDataItem\", col(\"CDCDATAITEM\")) \\\r\n",
							"                .withColumn(\"InspectionKey\", concat_ws(\"_\", col(\"HSISID\"), col(\"Year\"), col(\"Month\"), col(\"Day\"))) \\\r\n",
							"                .drop(\"NAME\") \\\r\n",
							"                .drop(\"ADDRESS1\") \\\r\n",
							"                .drop(\"ADDRESS2\") \\\r\n",
							"                .drop(\"CITY\") \\\r\n",
							"                .drop(\"STATE\") \\\r\n",
							"                .drop(\"POSTALCODE\") \\\r\n",
							"                .drop(\"PHONENUMBER\") \\\r\n",
							"                .drop(\"RESTAURANTOPENDATE\") \\\r\n",
							"                .drop(\"FACILITYTYPE\") \\\r\n",
							"                .drop(\"X\") \\\r\n",
							"                .drop(\"Y\") \\\r\n",
							"                .drop(\"GEOCODESTATUS\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivfinal.dtypes"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Similarly to inspections, we want to get inspection violations by month and write them into the `\\refined\\` folder."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_ivfinal.write.partitionBy(\"Year\", \"Month\") \\\r\n",
							"  .mode(\"overwrite\") \\\r\n",
							"  .parquet('abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/inspection_violations/')"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark Config Test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "notebookrunner",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7dd427e0-a0db-4ce0-82ea-d9ffb075c77c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
						"name": "notebookrunner",
						"type": "Spark",
						"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"azdata-precon-dev\")\r\n",
							"\r\n",
							"logger.info(\"Test simple log message\")\r\n",
							"\r\n",
							"import json\r\n",
							"msg = {\r\n",
							"    \"message\":\"Test simple log message\", \r\n",
							"    \"notebook\": \"Spark Config Test\", \r\n",
							"    \"source_data\": \"Wherever I get data\", \r\n",
							"    \"destination_data\": \"Wherever I want to send it\"\r\n",
							"}\r\n",
							"logger.info(json.dumps(msg))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Test Custom Code')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "notebookrunner",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "18beed24-2183-4b9f-bcb5-04172564bb79"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
						"name": "notebookrunner",
						"type": "Spark",
						"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from custompkg import add_numbers\r\n",
							"\r\n",
							"res = add_numbers(8, 6)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"res"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-spark-config')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.synapse.logAnalytics.enabled": "true",
					"spark.synapse.logAnalytics.workspaceId": "fc261f87-42dc-4a20-9da9-da133ba9c6d8",
					"spark.synapse.logAnalytics.keyVault.name": "vacctkv",
					"spark.synapse.logAnalytics.keyVault.linkedServiceName": "vacctkv",
					"spark.synapse.logAnalytics.keyVault.key.secret": "log-analytics-secret-key"
				},
				"created": "2022-11-02T12:59:15.5540000-04:00",
				"createdBy": "feaselkl@gmail.com",
				"configMergeRule": {
					"artifact.currentOperation.spark.synapse.logAnalytics.enabled": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.workspaceId": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.name": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/notebookrunner')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"libraryRequirements": {
					"content": "benford_py",
					"filename": "requirements.txt",
					"time": "2022-11-12T19:33:06.9493381Z"
				},
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "synapse-spark-config",
					"content": "{\"name\":\"synapse-spark-config\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.workspaceId\":\"fc261f87-42dc-4a20-9da9-da133ba9c6d8\",\"spark.synapse.logAnalytics.keyVault.name\":\"vacctkv\",\"spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"vacctkv\",\"spark.synapse.logAnalytics.keyVault.key.secret\":\"log-analytics-secret-key\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-11-02T12:59:15.5540000-04:00\",\"createdBy\":\"feaselkl@gmail.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.name\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret\":\"replace\"}}}",
					"time": "2022-11-12T19:33:06.9493381Z"
				},
				"sessionLevelPackagesEnabled": false,
				"customLibraries": [],
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}