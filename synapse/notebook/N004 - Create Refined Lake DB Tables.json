{
	"name": "004 - Create Refined Lake DB Tables",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "notebookrunner",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "58c46083-f113-4286-8b94-0427a5d41693"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/df403de8-b611-4a55-b347-93cebc510333/resourceGroups/synapse-precon/providers/Microsoft.Synapse/workspaces/azdata-precon-dev/bigDataPools/notebookrunner",
				"name": "notebookrunner",
				"type": "Spark",
				"endpoint": "https://azdata-precon-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/notebookrunner",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create Lake Database Tables\r\n",
					"\r\n",
					"A lake database is a construct in Azure Synapse Analytics which allows you to create tables based on existing data lake folders.  You can work with these tables as though they were normal tables, including data insertion, deletion, or updates.  In addition, you can query data in these tables from Spark pools or from the serverless SQL pool.\r\n",
					"\r\n",
					"You must create a lake database from Spark or from the **Data** tab."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"CREATE DATABASE inspection_lake"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Once you have created a database, you can create tables.  Here is the SQL syntax:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"DROP TABLE IF EXISTS inspection_lake.Restaurants;\r\n",
					"CREATE TABLE IF NOT EXISTS inspection_lake.Restaurants\r\n",
					"USING Parquet\r\n",
					"LOCATION 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/restaurants/*.parquet'"
				],
				"execution_count": 97
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"For partitioned tables like `Inspections`, we need to specify the partition strategy here, though unfortunately, we won't be able to take advantage of it in queries."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"DROP TABLE IF EXISTS inspection_lake.Inspections;\r\n",
					"CREATE TABLE IF NOT EXISTS inspection_lake.Inspections\r\n",
					"(\r\n",
					"    HSISID BIGINT,\r\n",
					"    Score INT,\r\n",
					"    DESCRIPTION STRING,\r\n",
					"    Type STRING,\r\n",
					"    Inspector STRING,\r\n",
					"    InspectionDate TIMESTAMP,\r\n",
					"    PermitID INT,\r\n",
					"    RestaurantName STRING,\r\n",
					"    InspectionKey STRING\r\n",
					")\r\n",
					"USING Parquet\r\n",
					"LOCATION 'abfss://synapse@azdatapreconstoredev.dfs.core.windows.net/wakeinspections/refined/inspections/Year=*/Month=*/*.parquet'\r\n",
					""
				],
				"execution_count": 92
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"We can query lake database tables either via PySpark (`spark.sql(\"SELECT ...\")`) or via the SQL magic syntax below."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT *\r\n",
					"FROM inspection_lake.Inspections\r\n",
					"WHERE\r\n",
					"    RestaurantName LIKE '%Tropical Picken Chicken%'\r\n",
					"ORDER BY\r\n",
					"    InspectionDate DESC\r\n",
					"LIMIT 100;"
				],
				"execution_count": 99
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"SELECT *\r\n",
					"FROM inspection_lake.Restaurants\r\n",
					"LIMIT 20;"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}